# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('creditcard.csv')

# Explore the data (this is where you would perform EDA)
# ...

# Preprocess the data
# Handle missing values if any
# ...

# Normalize the data
scaler = StandardScaler()
scaled_features = scaler.fit_transform(data.drop('Class', axis=1))

# Handle class imbalance
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(scaled_features, data['Class'])

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)

# Choose and train the model
# Logistic Regression
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)

# Random Forest Classifier
# rand_forest = RandomForestClassifier()
# rand_forest.fit(X_train, y_train)

# Make predictions
predictions = log_reg.predict(X_test)
# predictions = rand_forest.predict(X_test)

# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, predictions))

print("Confusion Matrix:")
conf_matrix = confusion_matrix(y_test, predictions)
sns.heatmap(conf_matrix, annot=True, fmt='d')
plt.show()

print("Accuracy Score:", accuracy_score(y_test, predictions))

# Further steps might include:
# - Hyperparameter tuning
# - Feature engineering
# - Trying different models and comparing their performance
